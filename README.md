# Ragment - Backend API

A powerful Python backend service for the Notebook LLM Clone application, featuring document processing, AI-powered retrieval, and real-time chat capabilities.

<img width="843" height="587" alt="Screenshot 2026-01-02 at 10 05 43‚ÄØPM" src="https://github.com/user-attachments/assets/5a536c48-5ba3-477c-9cc7-7be6523cbff6" />



## Features

- üìÑ **Advanced Document Processing** - Support for PDFs, images, and various document formats
- üîç **Hybrid Search** - Combines vector similarity and keyword search for optimal retrieval
- ü§ñ **AI-Powered RAG Pipeline** - Retrieval-Augmented Generation for accurate responses
- ‚ö° **Async Processing** - Background task processing with Celery
- üóÑÔ∏è **Vector Database** - Efficient embedding storage and retrieval with Supabase
- üîÑ **Redis Caching** - Fast data access and task queue management
- üéØ **Multi-Agent System** - Specialized agents for different tasks

## Tech Stack

- **Framework**: FastAPI (Python)
- **Task Queue**: Celery
- **Cache/Broker**: Redis
- **Database**: Supabase (PostgreSQL + pgvector)
- **Document Processing**: Poppler, Tesseract OCR, libmagic
- **Dependency Management**: Poetry
- **Testing**: pytest

## Project Structure

```
backend/
‚îú‚îÄ‚îÄ redis/                    # Redis data directory
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/         # Python cache files
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # AI agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models and schemas
‚îÇ   ‚îú‚îÄ‚îÄ rag/                 # RAG pipeline implementation
‚îÇ   ‚îú‚îÄ‚îÄ routes/              # API route handlers
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Business logic services
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utility functions
‚îÇ   ‚îî‚îÄ‚îÄ server.py            # Main FastAPI application
‚îú‚îÄ‚îÄ supabase/                # Supabase migrations and functions
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ .env                     # Environment variables (create from .env.sample)
‚îú‚îÄ‚îÄ .env.sample             # Environment variables template
‚îú‚îÄ‚îÄ .gitignore              # Git ignore rules
‚îú‚îÄ‚îÄ poetry.lock             # Poetry lock file
‚îú‚îÄ‚îÄ pyproject.toml          # Poetry configuration
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ start_redis.sh          # Redis startup script
‚îú‚îÄ‚îÄ start_server.sh         # API server startup script
‚îú‚îÄ‚îÄ start_worker.sh         # Celery worker startup script
‚îî‚îÄ‚îÄ stopAll.sh              # Stop all services script
```

## Prerequisites

- Python 3.9+
- Poetry (Python dependency manager)
- Redis
- Supabase account and local setup
- System dependencies: Poppler, Tesseract OCR, libmagic

## Setup

### 1. Install System Dependencies

These are required for document processing (PDFs, images, etc.)

**macOS:**

```bash
brew install poppler tesseract libmagic
```

**Linux (Ubuntu/Debian):**

```bash
sudo apt-get update
sudo apt-get install poppler-utils tesseract-ocr libmagic1
```

**Windows:**

- Download Poppler from [https://github.com/oschwartz10612/poppler-windows/releases](https://github.com/oschwartz10612/poppler-windows/releases)
- Download Tesseract from [https://github.com/UB-Mannheim/tesseract/wiki](https://github.com/UB-Mannheim/tesseract/wiki)
- Add both to your system PATH

### 2. Install Poetry

```bash
curl -sSL https://install.python-poetry.org | python3 -
```

### 3. Install Python Dependencies

```bash
poetry install
```

### 4. Set Up Supabase

Start Supabase locally:

```bash
npx supabase start
```

Get your credentials:

```bash
npx supabase status
```

### 5. Configure Environment Variables

Create a `.env` file:

```bash
cp .env.sample .env
```

Update the values in `.env` file:

```env
# Supabase Configuration
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_anon_key
SUPABASE_SERVICE_KEY=your_supabase_service_role_key  # Previously called "Secret Key"

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# AI Model Configuration
OPENAI_API_KEY=your_openai_api_key
# Add other LLM provider keys as needed

# Application Settings
ENVIRONMENT=development
DEBUG=true
```

> üí° **Tip:** Get your Supabase credentials by running `npx supabase status` after starting Supabase locally.
>
> ‚ö†Ô∏è **Note:** Supabase has updated their naming. The old variable `service_role key` is now simply called `Secret Key`.  
> üì∏ [Reference screenshot](https://ik.imagekit.io/5wegcvcxp/HarishNeel/supabase-credentials.png)

## Running the Application

You need to run **3 services** in separate terminal windows:

### Terminal 1: Start Redis üü•

```bash
sh start_redis.sh
```

This starts the Redis server for caching and task queue management.

### Terminal 2: Start API Server üü¶

```bash
sh start_server.sh
```

The API server will run on `http://localhost:8000`

**API Documentation:**
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Terminal 3: Start Celery Worker üü©

```bash
sh start_worker.sh
```

This processes background tasks including:
- Document ingestion
- Embedding generation
- Vector indexing
- Long-running operations

### Stop All Services

To stop everything at once:

```bash
sh stopAll.sh
```

This stops: Celery Worker, Redis Server, and API Server

## Development Tasks

### Complete the Basic Retrieval Pipeline

Every step is well documented inside the code. Follow the inline comments and TODOs.

### Database Schema Updates

1. **Update the initial schema:**
   - Insert your changes in the schema file before `(embedding vector_ip_ops);`
   - Insert additional changes after `(embedding vector_cosine_ops);`

2. **Create migration files:**
   - Create a new migration file for the Postgres function `vector_search_document_chunks`
   - Create a new migration file for the Postgres function `keyword_search_document_chunks`

### Complete the Advanced Retrieval Pipeline

Implement advanced retrieval techniques as outlined in the codebase documentation.

## API Endpoints

### Health Check
```
GET /health
```

### Document Management
```
POST   /api/documents/upload
GET    /api/documents
GET    /api/documents/{document_id}
DELETE /api/documents/{document_id}
```

### Chat & Query
```
POST   /api/chat
POST   /api/query
GET    /api/chat/history/{session_id}
```

### Projects
```
POST   /api/projects
GET    /api/projects
GET    /api/projects/{project_id}
PATCH  /api/projects/{project_id}
DELETE /api/projects/{project_id}
```

## Architecture

### RAG Pipeline

The Retrieval-Augmented Generation pipeline consists of:

1. **Document Ingestion** - Process and chunk documents
2. **Embedding Generation** - Create vector embeddings
3. **Vector Storage** - Store in Supabase with pgvector
4. **Hybrid Retrieval** - Combine vector and keyword search
5. **Context Ranking** - Rerank results for relevance
6. **Response Generation** - Generate answers using LLM

### Agent System

Specialized agents handle different tasks:
- **Query Agent** - Processes user queries
- **Document Agent** - Manages document operations
- **Synthesis Agent** - Combines information from multiple sources

## Testing

Run tests with pytest:

```bash
poetry run pytest
```

Run tests with coverage:

```bash
poetry run pytest --cov=src --cov-report=html
```

## Deployment

### Docker (Recommended)

```bash
docker build -t notebook-llm-backend .
docker run -p 8000:8000 notebook-llm-backend
```

### Production Considerations

- Use a production-ready ASGI server (Gunicorn + Uvicorn)
- Set up proper logging and monitoring
- Configure CORS settings appropriately
- Use environment-specific configuration
- Set up SSL/TLS certificates
- Configure rate limiting
- Implement proper authentication/authorization

## Troubleshooting

### Redis Connection Issues
```bash
# Check if Redis is running
redis-cli ping

# Should return: PONG
```

### Celery Worker Not Processing Tasks
```bash
# Check Celery logs
celery -A src.celery inspect active

# Restart the worker
sh stopAll.sh
sh start_worker.sh
```

### Document Processing Errors

Ensure system dependencies are installed:
```bash
# Test Poppler
pdftoppm -v

# Test Tesseract
tesseract --version
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow PEP 8 guidelines
- Use type hints
- Write docstrings for functions and classes
- Add unit tests for new features

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Built with FastAPI and Celery
- Powered by Supabase and pgvector
- Document processing with Poppler and Tesseract
- AI capabilities through OpenAI and other LLM providers

## Support

For support, please open an issue in the GitHub repository or contact the development team.

---

Built with üêç using Python and FastAPI
